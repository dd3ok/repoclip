import asyncio
import tempfile
import time
from fastapi import FastAPI, UploadFile, File, WebSocket, WebSocketDisconnect, Request, Header, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.concurrency import run_in_threadpool
from starlette.responses import FileResponse
from typing import Optional
from pathlib import Path

from .models import AnalyzeRequest, AnalyzeResponse, ExportRequest, ExportTextResponse
from .services import clone_repo_to_session, analyze_repo_path, unpack_zip_to_session
from .utils import safe_filename, session_dir, clean_session, collect_files_for_export, render_markdown_pages, ensure_safe_root

app = FastAPI(title="repo2md")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„¸ì…˜ë³„ë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë¥¼ ì €ìž¥
uploaded_paths = {}

SESSION_TTL_SECONDS = 300
CLEAN_INTERVAL_SECONDS = 300
cleanup_task = None
MD_PAGE_BYTES = 2 * 1024 * 1024

app.mount("/static", StaticFiles(directory="static"), name="static")


async def session_gc_loop():
    """ì„¸ì…˜ TTL ê¸°ë°˜ ë°±ê·¸ë¼ìš´ë“œ ì²­ì†Œ"""
    while True:
        try:
            root = ensure_safe_root()
            now = time.time()
            for d in root.iterdir():
                if not d.is_dir():
                    continue
                try:
                    mtime = d.stat().st_mtime
                except FileNotFoundError:
                    continue
                if now - mtime > SESSION_TTL_SECONDS:
                    clean_session(d.name)
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ì²­ì†Œ ì˜¤ë¥˜: {e}")
        await asyncio.sleep(CLEAN_INTERVAL_SECONDS)


@app.on_event("startup")
async def start_cleanup_task():
    global cleanup_task
    cleanup_task = asyncio.create_task(session_gc_loop())


@app.on_event("shutdown")
async def stop_cleanup_task():
    global cleanup_task
    if cleanup_task:
        cleanup_task.cancel()
        try:
            await cleanup_task
        except Exception:
            pass

@app.get("/")
def index():
    return FileResponse("static/index.html")

@app.get("/config")
def get_config(request: Request):
    base = f"{request.url.scheme}://{request.url.netloc}"
    return {"API_URL": base}


@app.head("/config")
def head_config(request: Request):
    base = f"{request.url.scheme}://{request.url.netloc}"
    return {"API_URL": base}

@app.websocket("/ws/{session_id}")
async def ws_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    print(f"ðŸ”Œ WebSocket ì—°ê²°: {session_id}")
    try:
        while True:
            msg = await websocket.receive_text()
            if msg == "ping":
                await websocket.send_text("pong")
            elif msg == "disconnect":
                print(f"ðŸ§¹ í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ìš”ì²­: {session_id}")
                await websocket.close()
                break
    except WebSocketDisconnect:
        print(f"ðŸ”Œ WebSocket ì—°ê²° í•´ì œ: {session_id}")
    except Exception as e:
        print(f"âŒ WebSocket ì˜¤ë¥˜: {e}")
        try:
            await websocket.close()
        except:
            pass
    finally:
        # ì—°ê²°ì´ ì–´ë–¤ ì´ìœ ë¡œë“  ì¢…ë£Œë˜ë©´ ì„¸ì…˜ í´ë” ì •ë¦¬
        clean_session(session_id)
        
        # ì—…ë¡œë“œëœ ZIP íŒŒì¼ì´ ìžˆë‹¤ë©´ ì‚­ì œ
        if session_id in uploaded_paths:
            upload_path = uploaded_paths[session_id]
            try:
                if upload_path.exists():
                    upload_path.unlink()
                    print(f"ðŸ—‘ï¸ ì—…ë¡œë“œëœ ZIP íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {upload_path}")
            except Exception as e:
                print(f"âŒ ì—…ë¡œë“œëœ ZIP íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            # íŒŒì¼ ê²½ë¡œ ì •ë³´ ì‚­ì œ
            del uploaded_paths[session_id]

        print(f"âœ… ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ: {session_id}")

@app.post("/analyze", response_model=AnalyzeResponse)
def analyze_repo(req: AnalyzeRequest, x_session_id: Optional[str] = Header(None)):
    if not x_session_id:
        raise HTTPException(status_code=400, detail="Missing X-Session-Id header")
    session_dir(x_session_id).touch(exist_ok=True)
    try:
        repo_path, repo_name = clone_repo_to_session(x_session_id, req.repo_url)
        data = analyze_repo_path(repo_path, repo_name)
        return data
    except RuntimeError as e:
        raise HTTPException(status_code=400, detail=str(e))

# ì—…ë¡œë“œ ìŠ¤íŠ¸ë¦¬ë° ì €ìž¥ ìœ í‹¸
async def save_upload_file(upload: UploadFile, dest: Path, chunk_size: int = 4 * 1024 * 1024) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    with dest.open("wb") as f:
        while True:
            chunk = await upload.read(chunk_size)
            if not chunk:
                break
            f.write(chunk)
    await upload.seek(0)

@app.post("/analyze_zip", response_model=AnalyzeResponse)
async def analyze_zip(file: UploadFile = File(...), x_session_id: Optional[str] = Header(None)):
    if not x_session_id:
        raise HTTPException(status_code=400, detail="Missing X-Session-Id header")
    session_base = session_dir(x_session_id)
    session_base.touch(exist_ok=True)

    # 1) ì—…ë¡œë“œ ZIPì„ ê³ ìœ  ì´ë¦„ìœ¼ë¡œ ì €ìž¥(ì„¸ì…˜ê³¼ ì—°ê²°)
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".zip", prefix=f"repo2md_{x_session_id}_") as tmp:
            upload_path = Path(tmp.name)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create temp file: {e}")

    uploaded_paths[x_session_id] = upload_path

    # 2) ì €ìž¥ (ìŠ¤íŠ¸ë¦¬ë°)
    try:
        await save_upload_file(file, upload_path)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save upload: {e}")

    # 3) ì €ìž¥ í™•ì¸
    if not upload_path.exists() or upload_path.stat().st_size == 0:
        raise HTTPException(status_code=500, detail="Uploaded ZIP not found after save")

    # 4) ì••ì¶• í•´ì œ ë° ë¶„ì„ (ìŠ¤ë ˆë“œí’€ë¡œ ì˜¤í”„ë¡œë“œ)
    try:
        repo_path, repo_name = await run_in_threadpool(unpack_zip_to_session, x_session_id, upload_path)
        data = await run_in_threadpool(analyze_repo_path, repo_path, repo_name)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to analyze zip: {e}")
    finally:
        # 5) ì—…ë¡œë“œ ZIP ì¦‰ì‹œ ì‚­ì œ(ìž„ì‹œ ë””ë ‰í„°ë¦¬ ì²­ì†Œ)
        try:
            if upload_path.exists():
                upload_path.unlink()
        except Exception:
            pass
        uploaded_paths.pop(x_session_id, None)

    return data


@app.post("/export/text", response_model=ExportTextResponse)
def export_text(req: ExportRequest, x_session_id: Optional[str] = Header(None)):
    if not x_session_id:
        raise HTTPException(status_code=400, detail="Missing X-Session-Id header")

    session_dir(x_session_id).touch(exist_ok=True)

    base = session_dir(x_session_id)
    repo_dir = base / req.repo_name
    if not repo_dir.exists():
        candidates = [d for d in base.iterdir() if d.is_dir()]
        if not candidates:
            raise HTTPException(status_code=400, detail="Repository not found in session")
        repo_dir = max(candidates, key=lambda d: d.stat().st_mtime)

    files = collect_files_for_export(repo_dir, req.dirs, req.exts, req.files)
    if not files:
        raise HTTPException(status_code=400, detail="No files matched the selection")
    pages = render_markdown_pages(req.repo_name, repo_dir, files, MD_PAGE_BYTES)
    return {
        "paginated": len(pages) > 1,
        "pages": pages,
        "page_size": MD_PAGE_BYTES,
        "total_pages": len(pages)
    }

@app.post("/export/file")
def export_file(req: ExportRequest, x_session_id: Optional[str] = Header(None)):
    if not x_session_id:
        raise HTTPException(status_code=400, detail="Missing X-Session-Id header")

    session_dir(x_session_id).touch(exist_ok=True)

    base = session_dir(x_session_id)
    repo_dir = base / req.repo_name
    if not repo_dir.exists():
        candidates = [d for d in base.iterdir() if d.is_dir()]
        if not candidates:
            raise HTTPException(status_code=400, detail="Repository not found in session")
        repo_dir = max(candidates, key=lambda d: d.stat().st_mtime)

    files = collect_files_for_export(repo_dir, req.dirs, req.exts, req.files)
    if not files:
        raise HTTPException(status_code=400, detail="No files matched the selection")
    pages = render_markdown_pages(req.repo_name, repo_dir, files, MD_PAGE_BYTES)
    filename = f"{req.repo_name}_export.md"

    def page_stream():
        for page in pages:
            yield page.encode("utf-8")

    return StreamingResponse(
        page_stream(),
        media_type="text/markdown; charset=utf-8",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'}
    )
